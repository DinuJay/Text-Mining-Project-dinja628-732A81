{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import emoji\n",
    "import tensorflow_hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dtw import *\n",
    "from sklearn.preprocessing import MaxAbsScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FUNCTIONS\n",
    "\n",
    "def get_sentiment_timeseries(data_f,column):\n",
    "\n",
    "    ####################\n",
    "    comp_list_news = []\n",
    "    for sentence in data_f[column]:\n",
    "        pol_scores = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        #print(\"{} {} {:}\".format(str( vs['compound']),\"  | <- compound score, the tweet -> \" ,sentence))\n",
    "        comp = pol_scores['compound']\n",
    "        if comp >= 0.05:\n",
    "            comp = 1\n",
    "        elif comp <= -0.05:\n",
    "            comp = -1\n",
    "        else:\n",
    "            comp = 0\n",
    "        comp_list_news.append(comp)\n",
    "        #print(\"{} {} {:}\".format(str( compound_score),\"| <- compound score, the tweet ->: \" ,sentence))\n",
    "    return comp_list_news\n",
    "\n",
    "\n",
    "def get_model_sentiment_timeseries(data_f,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import MaxAbsScaler\n",
    "    # read and drop nan values\n",
    "    pathh = 'Data/tweets/PrelabelData.csv'\n",
    "    df = pd.read_csv(pathh)\n",
    "    df = df.dropna()\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"sentiment\"], test_size=0.2)\n",
    "    # Vectorize the data\n",
    "    Cvec = CountVectorizer()\n",
    "    X_train_vectors, X_test_vectors = Cvec.fit_transform(X_train), Cvec.transform(data_f[column])\n",
    "    MAS = MaxAbsScaler()\n",
    "    X_train_scaled, X_test_scaled = MAS.fit_transform(X_train_vectors), MAS.transform(X_test_vectors)\n",
    "    # Use Logistic Regression  as the model to fit the training data\n",
    "    logreg_model = LogisticRegression()\n",
    "    logreg_model.fit(X_train_scaled, y_train)\n",
    "    # Predict onto the Twitter data\n",
    "    predictions = logreg_model.predict(X_test_scaled)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def get_bert_sentiment_timeseries(data_f,column):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # read and drop nan values\n",
    "    pathh = 'Data/tweets/PrelabelData.csv'\n",
    "    df = pd.read_csv(pathh).dropna()\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"sentiment\"], test_size=0.2)\n",
    "    # Use the BERT pretrained and fine tuned model to predict to the twitter data\n",
    "    predictions_bert = bert_model((data_f[column]))\n",
    "    # Extract the sentiment from the model and accumulate the sentiment values\n",
    "    save_list = []\n",
    "    print(\"-----\")\n",
    "    print(np.asarray(predictions_bert)[1])\n",
    "    for i in np.asarray(predictions_bert):\n",
    "        k = 0\n",
    "        prev_j = 0\n",
    "        save_k = 0\n",
    "        for j in i:\n",
    "            if j > prev_j:\n",
    "                save_k = k \n",
    "                prev_j = j\n",
    "            k = k + 1\n",
    "        save_list.append(save_k)\n",
    "    predictions = [x-1 for x in save_list]\n",
    "    return predictions\n",
    "\n",
    "def get_timeseries(compscore_list_tweets):    \n",
    "    cum_list_news = []\n",
    "    previous = 0\n",
    "    for i in range(len(compscore_list_tweets)):\n",
    "        temp_list = [compscore_list_tweets[i],previous]\n",
    "        cum_list_news.append(sum(temp_list))\n",
    "        previous = cum_list_news[i]\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    # Getting the trend line\n",
    "    X = np.linspace(0,len(cum_list_news),len(cum_list_news)).astype('int')\n",
    "    y = cum_list_news\n",
    "    T = np.stack([X, np.ones(len(X))]).T\n",
    "    model = LinearRegression()\n",
    "    trend_line=model.fit(np.asarray(X.reshape(-1, 1)),y)\n",
    "    coefficient = trend_line.coef_\n",
    "    line_intercept = trend_line.intercept_\n",
    "    mu=coefficient*X+line_intercept\n",
    "\n",
    "    new_y_vals = []\n",
    "    # Detrending\n",
    "    for i in range(len(X)):\n",
    "        new_y_vals.append(y[i]-mu[i])\n",
    "\n",
    "    return new_y_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import bz2\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from dtw import *\n",
    "\n",
    "def sentiment_stock(topic,dates,data_f,stock_file, sentmode):\n",
    "    # Filter the topics\n",
    "    sentence_list = data_f['text'].tolist()\n",
    "    topic_sentences = []\n",
    "    store_index = []\n",
    "    if topic != \"all\":\n",
    "        for sentence_row in sentence_list:\n",
    "          if re.search(topic, sentence_row, re.IGNORECASE):\n",
    "            topic_sentences.append(sentence_row)\n",
    "            store_index.append(sentence_list.index(sentence_row))\n",
    "    else:\n",
    "        for sentence_row in sentence_list:\n",
    "            topic_sentences.append(sentence_row)\n",
    "            store_index.append(sentence_list.index(sentence_row))\n",
    "\n",
    "\n",
    "    # new dataset with only topic_sentences and dates of index store_index\n",
    "    new_dates = [dates[i] for i in store_index]\n",
    "    topic_df = pd.DataFrame({'text': topic_sentences, 'datetime': new_dates})\n",
    "    topic_df = topic_df.sort_values(by=['datetime'])\n",
    "\n",
    "    # Call function depending on what model is selected with the filtered topics data\n",
    "    if sentmode == \"model\":\n",
    "        sent = get_model_sentiment_timeseries(topic_df, \"text\")\n",
    "    elif sentmode == \"vader\": \n",
    "        sent = get_sentiment_timeseries(topic_df, \"text\")\n",
    "    elif sentmode == \"bert\":\n",
    "        sent = get_bert_sentiment_timeseries(topic_df, \"text\")\n",
    "    time = get_timeseries(sent)\n",
    "    \n",
    "    from scipy import interpolate\n",
    "    ### Before smoothing\n",
    "    # add new_dates to time as the x axis\n",
    "    df = pd.DataFrame({'text'     : time, 'datetime'  : new_dates})\n",
    "    avg_df = df.groupby('datetime').mean()\n",
    "\n",
    "    # Load the stock data\n",
    "    df_stock = pd.read_csv(stock_file)\n",
    "    # Filter the dates by the days chosen topic is present\n",
    "    data_f_s = df_stock\n",
    "    dates = data_f_s['Date'].tolist()\n",
    "    # make object of years, months and days\n",
    "    dates = [datetime.strptime(date, '%m/%d/%Y') for date in dates]\n",
    "    # only years, months and days\n",
    "    dates = [date.date() for date in dates]\n",
    "    # Get only dates and price in a dataframe sorted by date\n",
    "    stock_df_new = pd.DataFrame({'price': df_stock['Close'], 'datetime': dates})\n",
    "    stock_df_new = stock_df_new.sort_values(by=['datetime'])\n",
    "\n",
    "    # New df with datesticke stock price and sentiment only for uniue dates\n",
    "    filt_stock = []\n",
    "    length_stock = len(stock_df_new)\n",
    "    dates_filt_stock = []\n",
    "    avg_df_filter = []\n",
    "    k = 0\n",
    "    for avgdate in avg_df.index:\n",
    "        for i in range(length_stock):\n",
    "            if stock_df_new['datetime'][i] == avgdate:\n",
    "                filt_stock.append(stock_df_new['price'][i])\n",
    "                dates_filt_stock.append(avgdate)\n",
    "                avg_df_filter.append(avg_df['text'][k])\n",
    "        k = k +1\n",
    "\n",
    "    mind = min(dates_filt_stock)\n",
    "    maxd = max(dates_filt_stock)\n",
    "    stock_df_new = pd.DataFrame({'datetime': dates_filt_stock, 'price': filt_stock, 'sentiment': avg_df_filter})\n",
    "\n",
    "    # Normalize \n",
    "    norm_list = stock_df_new.copy()\n",
    "    def norm(norm_column,max,min):\n",
    "        norm = (norm_column-min) / (max-min)\n",
    "        return norm\n",
    "    max_price = stock_df_new['price'].max()\n",
    "    min_price = stock_df_new['price'].min()\n",
    "    max_sentiment = stock_df_new['sentiment'].max()\n",
    "    min_sentiment = stock_df_new['sentiment'].min()\n",
    "    norm_list['price'] = norm(stock_df_new['price'],max_price,min_price)\n",
    "    norm_list['sentiment'] = norm(stock_df_new['sentiment'],max_sentiment,min_sentiment)\n",
    "\n",
    "    ######## DYNAMIC TIME WARPING\n",
    "    align = dtw(norm_list['sentiment'], norm_list['price'], keep_internals=True)\n",
    "    #align.plot(type=\"threeway\")\n",
    "    #plt.show()\n",
    "    dd = dtw(norm_list['sentiment'],norm_list['price'],  keep_internals=True, step_pattern=rabinerJuangStepPattern(2, \"c\"))\n",
    "    dd.plot(type=\"twoway\",offset=-1)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"data points: \", dd.jmin)\n",
    "    ### PEARSON\n",
    "    from scipy.stats import pearsonr\n",
    "    pearcorr, _ = pearsonr(norm_list['sentiment'], norm_list['price'])\n",
    "\n",
    "    ### EUCLID\n",
    "    euclid_dist = np.linalg.norm(norm_list['sentiment'] - norm_list['price'])\n",
    "    \n",
    "    ### BASELINE\n",
    "    n = len(norm_list['price'])\n",
    "    neutral_sent = [1] * n\n",
    "    euclid_dist_baseline = np.linalg.norm(neutral_sent - norm_list['price'])\n",
    "\n",
    "    return euclid_dist_baseline,euclid_dist,pearcorr, mind, maxd\n",
    "    ####ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    tweets = 'Data/tweets/tweets_from_elonmusk.csv'\n",
    "    tweets = pd.read_csv(tweets)\n",
    "    df_tweets = tweets\n",
    "    df_tweets.head()\n",
    "    data_f = df_tweets\n",
    "    dates = data_f['datetime'].tolist()\n",
    "    dates = [datetime.strptime(date, '%Y-%m-%d %H:%M:%S%z') for date in dates]\n",
    "    dates = [date.date() for date in dates]\n",
    "    data_f = pd.DataFrame({'text': data_f['text'].tolist(), 'datetime': dates})\n",
    "    data_f = data_f.sort_values(by=['datetime'])\n",
    "    # some default topics\n",
    "    topics = ['tesla','twitter','stock','dogecoin']\n",
    "    # stock data\n",
    "    stock_file = 'Data/stock/TSLA-marketwatch.csv'\n",
    "\n",
    "    return topics,dates,data_f,stock_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load bert model\n",
    "bert_model = tf.saved_model.load('my_model2')\n",
    "\n",
    "topics,dates,data_f,stock_file  = load_data() \n",
    "topics = ['all','twitter','tesla']\n",
    "\n",
    "for topic in topics:\n",
    "\n",
    "    print(\"---------------BERT-----------------------\")\n",
    "    euclid_dist_baseline, euclid, pearcorr, mind, maxd = sentiment_stock(topic,dates,data_f, stock_file, 'bert')\n",
    "    #print(\"For topic: \", topic, \" | and stock: \", stock_file)\n",
    "    print(\"start date: \", mind, \" end date: \", maxd)\n",
    "\n",
    "    print(\"Euclidean Baseline-Euclidean Distance: \", f'{(euclid_dist_baseline-euclid):.2f}')\n",
    "\n",
    "    #print(\"Pearson correlation: -1 to 1. 1 or -1 is linear correlation in minus or plus direction whilst 0 is no correlation\")\n",
    "    print(\"Pearson Correlation: \", f'{pearcorr:.2f}')  \n",
    "\n",
    "    print(\"---------------MODEL-----------------------\")\n",
    "    euclid_dist_baseline, euclid, pearcorr, mind, maxd = sentiment_stock(topic,dates,data_f, stock_file,  'model')\n",
    "    #print(\"For topic: \", topic, \" | and stock: \", stock_file)\n",
    "    print(\"start date: \", mind, \" end date: \", maxd)\n",
    "\n",
    "    print(\"Euclidean Baseline-Euclidean Distance: \", f'{(euclid_dist_baseline-euclid):.2f}')\n",
    "\n",
    "    #print(\"Pearson correlation: -1 to 1. 1 or -1 is linear correlation in minus or plus direction whilst 0 is no correlation\")\n",
    "    print(\"Pearson Correlation: \", f'{pearcorr:.2f}')\n",
    "\n",
    "    print(\"---------------VADER-----------------------\")\n",
    "    euclid_dist_baseline, euclid, pearcorr, mind, maxd = sentiment_stock(topic,dates,data_f, stock_file, 'vader')\n",
    "    #print(\"For topic: \", topic, \" | and stock: \", stock_file)\n",
    "    print(\"start date: \", mind, \" end date: \", maxd)\n",
    "\n",
    "    print(\"Euclidean Baseline-Euclidean Distance: \", f'{(euclid_dist_baseline-euclid):.2f}')\n",
    "\n",
    "    #print(\"Pearson correlation: -1 to 1. 1 or -1 is linear correlation in minus or plus direction whilst 0 is no correlation\")\n",
    "    print(\"Pearson Correlation: \", f'{pearcorr:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42d7d588f07bcd1231831f49487fe1b0ac510f544454ba91dee3d98e7dbd1e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
